{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3_-GuE0HUTw2",
    "outputId": "c427201d-32fc-44f9-e503-320329358a4a"
   },
   "outputs": [],
   "source": [
    "### This project was tested using both hierarchical clustering and Random Forest Regression for \n",
    "### selecting sensors used in prediction (dimensionality reduction), but tests conducted with\n",
    "### Random Forest Regression achieve significantly better results\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(178)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ajDm0SKXpwt"
   },
   "outputs": [],
   "source": [
    "# Finds the cluster items for a given column (sensor)\n",
    "\n",
    "def return_cluster_from_col(clusters_list: list) -> list:\n",
    "    cluster_index = None\n",
    "    for i, cluster in enumerate(clusters_list):\n",
    "        if col in cluster:\n",
    "            cluster_index = i\n",
    "      \n",
    "  \n",
    "    cluster_from_col = [col]\n",
    "    for item in clusters_list[cluster_index]:\n",
    "        if item != col:\n",
    "            cluster_from_col.append(item)\n",
    "    return cluster_from_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eSg3eldwUdjG"
   },
   "outputs": [],
   "source": [
    "# Finds the \"other\" sensors needed for predicting, using either clustering or feature importance\n",
    "\n",
    "def get_supporting_columns(index_name:str, clustering_algorithm: bool) -> list:\n",
    "    if clustering_algorithm:\n",
    "        with open(file_path + 'clusters_list.pkl', 'rb') as f:\n",
    "            clusters_list = pickle.load(f)\n",
    "        return [index_name] + return_cluster_from_col(clusters_list)\n",
    "  \n",
    "    else:\n",
    "        with open(file_path + 'importance_new/' + col + '_rank.pkl', 'rb') as f:\n",
    "            rank = pickle.load(f)[:11]\n",
    "        column_rank = []\n",
    "        for r in rank:\n",
    "            column_rank.append(df_temp.columns.values[r])\n",
    "        return [index_name] + column_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RhCN-V_re-zH"
   },
   "outputs": [],
   "source": [
    "def load_and_scale_data(cols_to_use: list):\n",
    "    df_load = pd.read_csv(file_path + 'interpolated_no_na_no_noise.csv', usecols=cols_to_use, sep=\";\")\n",
    "    df_load = df_load.loc[:, cols_to_use]\n",
    "    df_load.set_index('timestamp', inplace=True)\n",
    "    values = df_load.values\n",
    "    df_load = 0\n",
    "    scaled_values = scaler.fit_transform(values)\n",
    "    return scaled_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wNApEk0XaT8S"
   },
   "outputs": [],
   "source": [
    "# Creates 3 timesteps for the columns used for prediction\n",
    "\n",
    "def prepare_data(scaled_values: list) -> pd.DataFrame:\n",
    "    df_prep = pd.DataFrame(scaled_values)\n",
    "    columns = np.roll(df_prep.columns.values, -1)\n",
    "    df_prep = df_prep[columns]\n",
    "  \n",
    "    cols, names = list(), list()\n",
    "  \n",
    "    for i in range(lag_time_steps-1, -1, -1):\n",
    "        if (i == 0):\n",
    "            names += [('var%d(t)' % (j+1)) for j in df_prep.columns.values]\n",
    "            cols.append(df_prep.shift(i))\n",
    "        else:\n",
    "            names += [('var%d(t-%d)' % (j+1, i)) for j in columns[:-1]]\n",
    "            cols.append(df_prep[columns[:-1]].shift(i))\n",
    "    \n",
    "    prepared_data = pd.concat(cols, axis=1)\n",
    "    prepared_data.columns = names\n",
    "    prepared_data.dropna(inplace=True)\n",
    "    return prepared_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7u1_mkEHgFN4"
   },
   "outputs": [],
   "source": [
    "# Reshape because LSTM needs 3D input\n",
    "\n",
    "def get_train_and_test_data_reshaped(prepared_data: pd.DataFrame, n_features: int):\n",
    "    train = prepared_data.sample(frac=0.9, random_state=178)\n",
    "    test = prepared_data.drop(train.index)\n",
    "  \n",
    "    train, test = train.values, test.values\n",
    "  \n",
    "  \n",
    "    train_X, train_y = train[:, :-1], train[:, -1]\n",
    "    test_X, test_y = test[:, :-1], test[:, -1]\n",
    "  \n",
    "  \n",
    "    # reshape input to be 3D [samples, timesteps, features]\n",
    "    train_X = train_X.reshape((train_X.shape[0], lag_time_steps, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], lag_time_steps, n_features))\n",
    "  \n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pITP1knrjmTs"
   },
   "outputs": [],
   "source": [
    "def get_model(train_X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mse', 'mape'])\n",
    "  \n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PaWGXHjAjz0r"
   },
   "outputs": [],
   "source": [
    "def train_model(model_path, model, train_X, train_y, relative_path):\n",
    "    early_stopping_monitor = EarlyStopping(monitor='loss', mode='min', patience=1, verbose=1)\n",
    "\n",
    "    checkpoint = ModelCheckpoint(model_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    history = model.fit(train_X, train_y, epochs=n_epochs, batch_size=n_batch,\n",
    "                        validation_split=0.1, verbose=2,\n",
    "                        callbacks=[checkpoint])\n",
    "                        #callbacks=[early_stopping_monitor, checkpoint])\n",
    "                        #callbacks=[early_stopping_monitor])\n",
    "    with open(relative_path + 'history_' + str(n_epochs) + 'epochs.pkl', 'wb') as f_history:\n",
    "        pickle.dump(history, f_history, pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UpNxvQwUkE_e"
   },
   "outputs": [],
   "source": [
    "def predict_and_inverse_normalization(model, test_X, test_y, n_features, n_observations):\n",
    "  \n",
    "    # Make prediction\n",
    "    prediction = model.predict(test_X, verbose=1)\n",
    "  \n",
    "    print('feat', n_features)\n",
    "    print('obs', n_observations)\n",
    "    print('lag', lag_time_steps)\n",
    "  \n",
    "    # Invert prediction\n",
    "    test_X = test_X.reshape((test_X.shape[0], n_observations))\n",
    "  \n",
    "    concated_prediction = np.concatenate((prediction, test_X[:, -n_features:]), axis=1)\n",
    "    inverse_prediction = scaler.inverse_transform(concated_prediction)[:, 0]\n",
    "  \n",
    "    # Invert scaling for actual values\n",
    "    test_y = test_y.reshape((len(test_y), 1))\n",
    "    concated_actual = np.concatenate((test_y, test_X[:, -n_features:]), axis=1)\n",
    "    inverse_actual = scaler.inverse_transform(concated_actual)[:, 0]\n",
    "  \n",
    "    return inverse_prediction, inverse_actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a5cBRKqPoZKO"
   },
   "outputs": [],
   "source": [
    "def plot(prediction, actual, history, save: bool, relative_path: str, rmse: float, is_clustering: bool):\n",
    "  \n",
    "    fig = plt.figure(facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_xlabel('Epoch')\n",
    "    if is_clustering:\n",
    "        ax.set_title('Loss - Clustering')\n",
    "    else:\n",
    "        ax.set_title('Loss - Feature Importance')\n",
    "    ax.plot(history.history['loss'], label='Training')\n",
    "    ax.plot(history.history['val_loss'], label='Validation')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    fig2 = plt.figure(facecolor='white')\n",
    "    ax2 = fig2.add_subplot(111)\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    if is_clustering:\n",
    "        ax2.set_title('MAPE - Clustering')\n",
    "    else:\n",
    "        ax2.set_title('MAPE - Feature Importance')\n",
    "    ax2.plot(history.history['mean_absolute_percentage_error'], label='Mean Absolute Percentage Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    fig3 = plt.figure(facecolor='white')\n",
    "    ax3 = fig3.add_subplot(111)\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    if is_clustering:\n",
    "        ax3.set_title('MSE - Clustering')\n",
    "    else:\n",
    "        ax3.set_title('MSE - Feature Importance')\n",
    "  \n",
    "    ax3.plot(history.history['mean_squared_error'], label='Mean Squared Error')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    fig4 = plt.figure(facecolor='white')\n",
    "    ax4 = fig4.add_subplot(111)\n",
    "    ax4.set_xlabel('Timestamp Index')\n",
    "    if is_clustering:\n",
    "        ax4.set_title('Clustering - RMSE = ' + str(rmse))\n",
    "    else:\n",
    "        ax4.set_title('Feature Importance - RMSE = ' + str(rmse))\n",
    "    ax4.plot(actual, label='Actual Data')\n",
    "    ax4.plot(prediction, label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    fig5 = plt.figure(facecolor='white')\n",
    "    ax5 = fig5.add_subplot(111)\n",
    "    ax5.plot(actual, label='True Data')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "  \n",
    "    if save:\n",
    "      \n",
    "        loss_path = relative_path + 'loss_' + str(n_epochs) + '_epochs.png'\n",
    "        fig.savefig(loss_path)\n",
    "\n",
    "        mape_path = relative_path + 'mape_' + str(n_epochs) + '_epochs.png'\n",
    "        fig2.savefig(mape_path)\n",
    "\n",
    "        mse_path = relative_path + 'mse_' + str(n_epochs) + '_epochs.png'\n",
    "        fig3.savefig(mse_path)\n",
    "\n",
    "        pred_path = relative_path + 'prediction_' + str(n_epochs) + '_epochs.png'\n",
    "        fig4.savefig(pred_path)\n",
    "    \n",
    "        if not is_clustering:\n",
    "            actual_path = relative_path + 'actual_' + str(n_epochs) + '_epochs.png'\n",
    "            fig5.savefig(actual_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aYtx3_3nsU4_"
   },
   "outputs": [],
   "source": [
    "def run_full(cols_to_use: list, relative_path: str):\n",
    "    n_features = len(cols_to_use) - 2\n",
    "    n_observations = n_features * lag_time_steps\n",
    "  \n",
    "    model_path = relative_path + 'model_' + str(n_epochs) + '_epochs.h5'\n",
    "\n",
    "    scaled_values = load_and_scale_data(cols_to_use)\n",
    "    prepared_data = prepare_data(scaled_values)\n",
    "    train_X, train_y, test_X, test_y = get_train_and_test_data_reshaped(\n",
    "        prepared_data, n_features)\n",
    "\n",
    "    model = get_model(train_X)\n",
    "\n",
    "    history = train_model(model_path, model, train_X, train_y, relative_path)\n",
    "  \n",
    "    model = load_model(model_path)\n",
    "  \n",
    "    history_path = relative_path + 'history_' + str(n_epochs) + 'epochs.pkl'\n",
    "    with open(history_path, 'rb') as f_history:\n",
    "        history = pickle.load(f_history)\n",
    "\n",
    "    prediction, actual = predict_and_inverse_normalization(\n",
    "        model, test_X, test_y, n_features, n_observations)\n",
    "\n",
    "\n",
    "    return prediction, actual, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5bHJQ2eizRck"
   },
   "outputs": [],
   "source": [
    "def run_full_cluster_and_importance(index_name: str, relative_path: str, save_plot: bool, start_from_cluster: bool):\n",
    "    predictions = []\n",
    "    actual_values = []\n",
    "    histories = []\n",
    "  \n",
    "    for i in range(1):\n",
    "        is_clustering = False\n",
    "        if start_from_cluster:\n",
    "            start_from_cluster = False\n",
    "            continue\n",
    "        \n",
    "        if i == 0:\n",
    "            cols_to_use = get_supporting_columns(index_name, False)\n",
    "      \n",
    "        else:\n",
    "            is_clustering = True\n",
    "            cols_to_use = get_supporting_columns(index_name, True)\n",
    "            relative_path += 'clustering_'\n",
    "    \n",
    "        print('cols to use')\n",
    "        print(cols_to_use)\n",
    "      \n",
    "        prediction, actual, history = run_full(cols_to_use, relative_path)\n",
    "        predictions.append(prediction)\n",
    "        actual_values.append(actual)\n",
    "        histories.append(history)\n",
    "    \n",
    "        rmse = sqrt(mean_squared_error(prediction, actual))\n",
    "\n",
    "        print('Test RMSE: %.3f' % rmse)\n",
    "        print(prediction)\n",
    "        print(actual)\n",
    "    \n",
    "        plot(prediction, actual, history, save_plot, relative_path, rmse, is_clustering)\n",
    "    \n",
    "    return predictions, actual_values, histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 717
    },
    "colab_type": "code",
    "id": "avyJaYodyl5B",
    "outputId": "ff67473e-d619-4e8c-9ac2-1fbb9b76a052"
   },
   "outputs": [],
   "source": [
    "scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "df = pd.read_csv('interpolated_no_na_no_noise.csv', index_col=0, sep=\";\")\n",
    "\n",
    "\n",
    "lag_time_steps = 3\n",
    "\n",
    "n_epochs = 200\n",
    "n_batch = 72 \n",
    "\n",
    "start_from_cluster = False # Incase the feature importance worked well, but clustering needs to be run again\n",
    "\n",
    "\n",
    "dir_cols = df.columns.values\n",
    "\n",
    "predictions_both_methods = [] # Both methods referring to clustering and feature importance\n",
    "actual_values_both_methods = []\n",
    "histories_both_methods = []\n",
    "\n",
    "\n",
    "for col in dir_cols:\n",
    "  \n",
    "    print('The col is ' + str(col))\n",
    "\n",
    "    relative_path = file_path + 'models/plots/' + str(col) + '/'\n",
    "\n",
    "    save_plot = True\n",
    "\n",
    "    prediction_both_methods, actual_value_both_methods, history_both_methods = \n",
    "    run_full_cluster_and_importance(df_temp.index.name, relative_path, save_plot, start_from_cluster)\n",
    "    \n",
    "    predictions_both_methods.append(prediction_both_methods)\n",
    "    actual_values_both_methods.append(actual_value_both_methods)\n",
    "    histories_both_methods.append(history_both_methods)\n",
    "  \n",
    "    print('Finished col ' + str(col))\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------')\n",
    "    print('-----------------------------------------------------------------')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "lstm_refactored.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
